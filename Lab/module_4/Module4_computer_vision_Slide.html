<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Urban Images and Computer Vision</title>
    <meta charset="utf-8" />
    <meta name="author" content="Originally written by Bon Woo Koo &amp; Subhro Guhathakurta; modified by Uijeong Hwang" />
    <script src="libs/header-attrs-2.23/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="assets/sydney-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/sydney.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

class: middle, inverse

# Urban Images and Computer Vision

.font100[
Subhro Guhathakurta &amp; Uijeong Hwang

10/19/2023
]





---
## Module 4 in a nutshell

1. Get Google API key.
2. Download street network data (OSM) and clean it.
3. Generate points along the edges. These will be where GSV images will be downloaded.
4. Calculate the heading of the cameras for each point. 
5. Create a function that takes a point as input and download GSV images.
6. Apply computer vision to the images.
7. Merge the results back to the points from Step 2.
* We will cover Steps 1-5 this week and Steps 6-7 next week.

---
## Why Street View Images

* Measuring built environment is important for numerous reasons - public health, public safety, environmental sustainability, economic vitality, tourism, etc.
* Built environment literature commonly differentiate between the urban form and streetscape.
  * ***Urban form***: macroscale-built environment which characterizes street connectivity, density, and diversity of land use.
  * ***Streetscape***: pertains to physical layout and design of the built environment, such as **sidewalk width, enclosure, tree canopy, landscaping, and street furniture**.
* Street View Images allow us to examine the streetscapes without traveling. 

---
## Google Street View Images 

* Images taken at roughly 10 meter intervals from cameras that are (often but not always) mounted on car roof.
* 360-degree image in all directions.
* Have coverage both in US and internationally and can go back in time. 

&lt;img src="img/street_view_car.png" width="50%" /&gt;
  
.footnotesize[Source: Google]

---
## Google Street View Images

* Around 2010, planning studies started using Google Street View (GSV) images.
* To audit street environments using GSV images.
* In early studies, still human auditors were looking at GSV and did manual audits.
* Recent studies are increasingly using computer vision instead of manual audits.

---
## Google Street View Images

* Web version of GSV is free but **.red[their API is NOT FREE!]** (7.00 USD per 1000 images)
* Maximum Queries per Minute (QPM): 30,000. Requests up to 25,000 per day require an API key.
* Do not distribute images nor use it for commercial purposes.
* You should **.red[NEVER EXPOSE]** your API key. You can get charged for a lot of money.
* Get your key [here](https://raw.githubusercontent.com/ujhwang/UrbanAnalytics2023/main/Lab/module_4/how_to_get_your_key.pptx)

---

.pull-left[
.center[
**GSV from web**
]
![web](img/gsv_web.JPG)
]

.pull-right[
.center[
**GSV from API**
]
![api](img/gsv_api.jfif)
]

---
## Example Request

.center[
https://maps.googleapis.com/maps/api/streetview?.red[size=600x300].blue[&amp;location=46.414382,10.013988]&lt;br&gt;.orange[&amp;heading=151.78].pink[&amp;pitch=-0.76].green[&amp;key=YOUR_API_KEY]
]

* **Size**: Cappted at 640x 640 pixels.
* **Location**: Coordinates in 4326.
* **Heading**: Heading of the camera &lt;br&gt;.small[.gray[(0=North, 90=East, 180=South, 270=West, 360=North)]]
* **Pitch** (default 0): Specified up or down angle of the camera.
* **Source**:  limits Street View searches to selected sources. Either default or outdoor.
* **fov**: (default 90): determines the horizontal field of view of the image.
* **key**: Your API key.

---

## Try it

This URL (with your key added) will give you an image at Tech Square.

https://maps.googleapis.com/maps/api/streetview?size=640x640&gt;&amp;location=33.7768249,-84.388767&amp;heading=224.96&amp;&lt;br&gt;fov=90&amp;pitch=0&amp;key=YOUR_API_KEY

---
## Computer vision
* Computer vision is the field of AI (particularly, deep learning) that enables computers to interpret and understand visual information.
* Deep learning is part of a broader family of AI methods based on **artificial neural networks** with representation learning.
* Example architecture: Convolutional Neural Network (CNN)
![](img/cnn.jpg)
  
.footnotesize[
(source: https://medium.com/@eric.perbos/fast-ai-deep-learning-for-coders-part-1-2017-3db56c1a4cf3)
]

---
## Computer vision
* Deep learning algorithms are stacked in a hierarchy of increasing complexity and abstraction
  
![](img/stacked_complexity.png)

.footnotesize[
(source: https://medium.com/analytics-vidhya/convolutional-neural-network-an-informal-intro-part-1-db9fca86a750)
]

---
## Classification
* Image classification: [3D interactive visualization of CNN](https://adamharley.com/nn_vis/cnn/3d.html) (source: adamharley.com) 
![](img/digit_classification.png)

---
## Abstract task
* Predicting perceptual attribute: visual safety score of street view images
  
![](img/visual_safety_score.png)

---
## Segmentation
* Semantic Segmentation: Classifies each pixel of an image into a class.
* Instance Segmentation: Detects objects and distinguishes instances.
* Panoptic Segmentation: Combines the two methods above.
&lt;img src="img/segmentation_types.png" width="73%" /&gt;
.footnotesize[
(source: https://arxiv.org/pdf/2006.12567.pdf)
]

---
## Google Colab(oratory)

* Google Colab is a cloud-based Jupyter notebook service hosted by Google.
* The free tier has 12 hour limit; after 12 hours of computation, your session will expire.

&lt;img src="img/colab_option.png" width="70%" /&gt;
  
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "4:3"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
