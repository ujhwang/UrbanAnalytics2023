---
title: "Major Assignment 3"
author: "Bonwoo Koo & Subhrajit Guhathakurta"
date: '2022-11-13'
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
---

<style type="text/css">
  body{
  font-family: Arial;
  }
</style>


# How to use this template

You will see # TASK ///// through out this template. This indicates the beginning of a task.  Right below it will be instructions for the task.
Each # TASK ///// will be paired with # //TASK ///// to indicate where that specific task ends.

For example, if you need something like below...
```{r example}
# TASK ////////////////////////////////////////////////////////////////////////
# create a vector with element 1,2,3 and assign it into `my_vec` object
# **YOUR CODE HERE..**
# //TASK //////////////////////////////////////////////////////////////////////
```

What I expect you to do is to replace where it says `# **YOUR CODE HERE..**` with your answer, like below.
```{r}
# TASK ////////////////////////////////////////////////////////////////////////
# create a vector with element 1,2,3 and assign it into `my_vec` object
my_vec <- c(1,2,3)
# //TASK //////////////////////////////////////////////////////////////////////
```


There can be multi-step instructions, like shown below. You may use pipe (%>%) to link multiple functions to perform the task in the instruction. Make sure that **you assign the output of your task into an object with the specified name**. This is to make sure that your code will run smoothly - if you change the name of the object (i.e., subset_car in the example below), all the subsequent code will NOT run properly.
```{r}
# TASK ////////////////////////////////////////////////////////////////////////
# 1. Using mtcars object, extract rows where cyl equals 4
# 2. Select mpg and disp columns
# 3. Create a new column 'summation' by adding mpg and disp
# 4. assign it into `subset_car` object
subset_car <- # **YOUR CODE HERE..**
# //TASK //////////////////////////////////////////////////////////////////////
```

I expect you to repalce where it says `# **YOUR CODE HERE..**` with your answer, like below.
```{r}
# TASK ////////////////////////////////////////////////////////////////////////
# 1. Using mtcars object, extract rows where cyl equals 4
# 2. Select mpg and disp columns
# 3. Create a new column 'summation' by adding mpg and disp
# 4. assign it into `subset_car` object
subset_car <- mtcars %>% 
  filter(cyl == 4) %>% 
  select(mpg, disp) %>% 
  mutate(summation = mpg + disp)
# //TASK //////////////////////////////////////////////////////////////////////
```

There will also be multiple 'NO MODIFICATION ZONE'. Do not modify code in the No Modification Zone.

> You will need to knit it, publish it on Rpubs, and submit the link. If there is any question about this template, do not hesitate to reach out to Bonwoo.

# Introduction

In this assignment, we will donwload Tweets that contain the names of neighborhoods in Atlanta. We will apply sentiment analysis to the Tweets and map/plot the sentiments associated with neighborhoods. Specifically, you will be performing the following steps:

Step 1. You will download and read a shapefile that contains neighborhood boundary and thier names.
Step 2. Initiate a deep learning-based package for sentiment analysis called "sentiment.ai" (if you have problem with this package, you can use a different package).
Step 3. Loop through the names of neighborhoods in Atlanta to collect Tweets.
Step 4. Clean and filter the collected Tweets.
Step 5. Analyze the Tweets.

**As always, load packages first.**
```{r library, message=FALSE, warning=FALSE,tidy=TRUE}
library(rtweet)
library(tidyverse)
library(sf)
library(sentiment.ai)
library(SentimentAnalysis)
library(ggplot2)
library(here)
library(tmap)
```

# Step 1. Neighborhood Shapefile

Go to [this webpage](https://opendata.atlantaregional.com/datasets/coaplangis::atlanta-neighborhoods/about) and download the shapefile from there. Once downloaded, read the data into your current R environment.

```{r Read neighborhood shapefile}
# TASK ////////////////////////////////////////////////////////////////////////

# Read neighborhood shapefile
nb_shp <- st_read("your-path-to-file-here")

# //TASK //////////////////////////////////////////////////////////////////////
```



# Step 2. Initiate Sentiment.ai

If you have issues with using this package, you can use the other package introduced in the class called SentimentAnalysis.

```{r Initiating sentiment ai, message=FALSE, warning=FALSE}
# TASK ////////////////////////////////////////////////////////////////////////

# Initiate sentiment.ai 
init_sentiment.ai(envname = "r-sentiment-ai", method = "conda") # feel free to change these arguments if you need to.

# //TASK //////////////////////////////////////////////////////////////////////
```

# Step 3. Looping through neighborhood names to get Tweets

Prepare to use Twitter API by specifying arguments of create_token() function using your credentials.

```{r Twitter key setting, message=FALSE, warning=FALSE}
# TASK ////////////////////////////////////////////////////////////////////////

# whatever name you assigned to your created app
appname <- "your-app-name"

# create token named "twitter_token"
# the keys used should be replaced by your own keys obtained by creating the app  
twitter_token <- create_token(
 app = appname,
  consumer_key = "your_key_here", 
  consumer_secret = "your_key_here",
  access_token = "your_key_here",
  access_secret = "your_key_here")

# //TASK //////////////////////////////////////////////////////////////////////
```

Next, let's define a function that downloads Tweets, clean them, and apply sentiment analysis to them.

```{r Function Definition}
# Extract neighborhood names from nb_shp's NAME column and store it in nb_names object.
nb_names <- nb_shp$NAME

# Define a search function
get_twt <- function(term){
  # =========== NO MODIFICATION ZONE STARTS HERE ===============================
  term_mod <- paste0("\"", term, "\"")
  # =========== NO MODIFY ZONE ENDS HERE ========================================

  
  # TASK ////////////////////////////////////////////////////////////////////////
  
  # 1. Use search_tweets() function to get Tweets.
  #    Use term_mod as the search keyword to get Tweets.
  #    Set n to a number large enough to get all Tweets from the past 7 days
  #    Set geocode argument such that the search is made with 50 mile radius from 33.76, -84.41
  #    Be sure the exlucde retweets.
  #    You may need to enable the function to automatically wait if rate limit is exceeded.
  #    I recommend using suppressWarnings() to suppress warnings.
  #    Make sure you assign the output from the seach_tweets to object named 'out'
  
  out <- # **YOUR CODE HERE..**
    
  # //TASK //////////////////////////////////////////////////////////////////////
  
  
  
  # =========== NO MODIFICATION ZONE STARTS HERE ===============================
  out <- out %>%
    select(created_at, id, id_str, full_text, geo, coordinates, place, text) 

  
  # Basic cleaning
  replace_reg <- "http[s]?://[A-Za-z\\d/\\.]+|&amp;|&lt;|&gt;"

  out <- out %>% 
    mutate(text = str_replace_all(text, replace_reg, ""),
           text = gsub("@", "", text),
           text = gsub("\n\n", "", text))
  
  # Sentiment analysis
  # Also add a column for neighborhood names
  if (nrow(out)>0){
    out <- out %>% 
      mutate(sentiment_ai = sentiment_score(out$text),
             sentiment_an = analyzeSentiment(text)$SentimentQDAP,
             nb = term)
    print(paste0("Search term:", term))
  } else {
    return(out)
  }
  
  return(out)
}
# =========== NO MODIFY ZONE ENDS HERE ========================================
```

**<font color=pink> Let's apply the function to Tweets. Note that this code chunk may take more than 15 minutes if you've already spent some (or all) of your rate limit. </font>**
```{r get_twt}
# =========== NO MODIFICATION ZONE STARTS HERE ===============================
# Apply the function to get Tweets
twt <- map(nb_names, ~get_twt(.x))
# =========== NO MODIFY ZONE ENDS HERE ========================================
```

## Step 4. Clean and filter the collected Tweets.

The downloaded Tweets need some cleaning / reorganizing process, including

1. Drop empty elements from the list `twt`. These are neighborhoods with no Tweets referring to them. Hint: you can create a logical vector that has FALSEs if the corresponding elements in `twt` has no Tweets and TRUE otherwise.

2. The `coordinates` column is currently a list-column. Unnest this column so that lat, long, and type (i.e., column names inside coordinates) are separate columns. You can use unnest() function.

3. Calculate the average sentiment score for each neighborhood. You can group_by() `nb` column in twt objects and summarise() to calculate means. Also add an additional column `n` that contains the number of rows in each group using n() function.

4. Join the cleaned Tweet data back to the neighborhood shapefile. Use the neighborhood name as the join key. **Make sure that the result of the join is assigned to an object called `twt_poly` to ensure that the subsequent code runs smoothly.**

```{r Merge Tweets with shapefile}
# No code is provided as a template. Feel free to write your own code to perform the tasks listed above. 
# MAKE SURE THAT THE LAST RESULT IS ASSIGNED TO AN OBJECT NAMED `twt_poly`.
```


## Step 5. Analysis

Now that we have collected Tweets, calculated sentiment score, and merged it back to the original shapefile, we can map them to see spatial distribution and draw plots to see inter-variable relationships.

First, let's draw two interactive choropleth maps, one using sentiment score as the color and the other one using the number of Tweets as the color. Use tmap_arrange() function to display the two maps side-by-side.

```{r}
# No code is provided as a template. 
# Feel free to write your own code to perform the tasks listed above. 
```

Second, Use ggplot 2 package to draw a scatterplot using the number of Tweets for each neighborhood on X-axis and sentiment score on Y-axis. Also perform correlation analysis between the number of Tweets for each neighborhood and sentiment score either using cor.test() function or ggpubr::stat_cor() function.

```{r}
# No code is provided as a template. 
# Feel free to write your own code to perform the tasks listed above. 
```

Using the map and plot you created above (as well as using your inspection of the data), answer the following questions.

Q. What's the proportion of neighborhoods with one or more Tweets?
Q. Do you see any pattern to neighborhoods with/without Tweets? Is there anything that can help us guess how likely a given neighborhood will have Tweets?
Q. (If you've observed relationship between sentiment score and the number of Tweets) Why do you think there is the relationship between sentiment score and the number of Tweets?
Q. The neighborhood 'Rockdale' has many Tweets mentioning its name. Does high volume of Tweets make sense? Why do you think this occurred?
Q. What do you think are the strengths and shortcomings of this method (i.e., using Twitter & neighborhood names to evaluate sentiments around each neighborhood)?
Q. Can you think of a better way to define neighborhoods and collect Tweets that can better represent the sentiment of neighborhoods?  